{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSK_1_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk1.txt\"\n",
    "HSK_2_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk2.txt\"\n",
    "HSK_3_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk3.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def convert_tsv_to_txt(tsv_path, output_path):\n",
    "    chinese_sentences = []\n",
    "    \n",
    "    # Read TSV file using csv module\n",
    "    with open(tsv_path, 'r', encoding='utf-8') as tsv_file:\n",
    "        tsv_reader = csv.reader(tsv_file, delimiter='\\t')\n",
    "        for row in tsv_reader:\n",
    "            if len(row) >= 3 and row[1] == 'cmn':  # Check if row has enough columns and language is 'cmn'\n",
    "                chinese_sentences.append(row[2])\n",
    "    \n",
    "    # Write to output file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in chinese_sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "    \n",
    "    print(f\"Converted {len(chinese_sentences)} Chinese sentences to {output_path}\")\n",
    "\n",
    "def read_hsk_vocabulary(path):\n",
    "\n",
    "    \"\"\"\n",
    "    Read HSK vocabulary words from a file and return them as a list.\n",
    "    \n",
    "    Args:\n",
    "        path (str): Path to the HSK vocabulary file\n",
    "        \n",
    "    Returns:\n",
    "        list: List of HSK vocabulary words\n",
    "    \"\"\"\n",
    "    vocabulary = []\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Strip whitespace and skip empty lines\n",
    "                word = line.strip()\n",
    "                if word:\n",
    "                    vocabulary.append(word)\n",
    "        print(f\"Successfully loaded {len(vocabulary)} words from {path}\")\n",
    "        return vocabulary\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading vocabulary file: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "def filter_sentences_by_hsk_vocabulary(hsk_vocabulary, sentences_path):\n",
    "    \"\"\"\n",
    "    Filter Chinese sentences to keep only those containing characters from the given HSK vocabulary.\n",
    "    \n",
    "    Args:\n",
    "        hsk_vocabulary (list): List of HSK vocabulary words\n",
    "        sentences_path (str): Path to the file containing Chinese sentences\n",
    "        \n",
    "    Returns:\n",
    "        list: List of filtered sentences that only contain HSK vocabulary characters\n",
    "    \"\"\"\n",
    "    # Create a set of all characters from HSK vocabulary for faster lookups\n",
    "    hsk_chars = set()\n",
    "    for word in hsk_vocabulary:\n",
    "        hsk_chars.update(word)\n",
    "    \n",
    "    filtered_sentences = []\n",
    "    total_sentences = 0\n",
    "    \n",
    "    try:\n",
    "        with open(sentences_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                sentence = line.strip()\n",
    "                if not sentence:\n",
    "                    continue\n",
    "                    \n",
    "                total_sentences += 1\n",
    "                \n",
    "                # Check if all characters in the sentence are in HSK vocabulary\n",
    "                all_chars_in_hsk = True\n",
    "                for char in sentence:\n",
    "                    # Skip non-Chinese characters (punctuation, spaces, etc.)\n",
    "                    if '\\u4e00' <= char <= '\\u9fff':  # Check if character is Chinese\n",
    "                        if char not in hsk_chars:\n",
    "                            all_chars_in_hsk = False\n",
    "                            break\n",
    "                \n",
    "                if all_chars_in_hsk:\n",
    "                    filtered_sentences.append(sentence)\n",
    "        \n",
    "        print(f\"Processed {total_sentences} sentences\")\n",
    "        print(f\"Found {len(filtered_sentences)} sentences containing only HSK vocabulary characters\")\n",
    "        return filtered_sentences\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing sentences file: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_chinese_characters(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract only Chinese characters from a file with format: 'chinese\\tpinyin\\tenglish\\tHSK level'\n",
    "    \n",
    "    Args:\n",
    "        input_path (str): Path to the input file\n",
    "        output_path (str): Path to save the output file with only Chinese characters\n",
    "    \"\"\"\n",
    "    chinese_words = []\n",
    "    \n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                # Split by tab and take the first element (Chinese characters)\n",
    "                parts = line.strip().split('\\t')\n",
    "                if parts and parts[0]:  # Check if line is not empty and has Chinese characters\n",
    "                    chinese_words.append(parts[0])\n",
    "        \n",
    "        # Write to output file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for word in chinese_words:\n",
    "                f.write(word + '\\n')\n",
    "        \n",
    "        print(f\"Successfully extracted {len(chinese_words)} Chinese words to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 72171 Chinese sentences to ../../datasets/raw/cmn_sentences.txt\n"
     ]
    }
   ],
   "source": [
    "convert_tsv_to_txt(\"../../datasets/raw/cmn_sentences.tsv\", \"../../datasets/raw/cmn_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 686 Chinese words to ../../datasets/vocabulary/hsk3.txt\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "extract_chinese_characters(\"../../datasets/vocabulary/hsk/hsk3.txt\", \"../../datasets/vocabulary/hsk3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 130 words from ../../datasets/vocabulary/hsk1.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['爱',\n",
       " '爸爸',\n",
       " '杯子',\n",
       " '北京',\n",
       " '本',\n",
       " '不错',\n",
       " '菜',\n",
       " '茶',\n",
       " '吃',\n",
       " '出租车',\n",
       " '打电话',\n",
       " '大',\n",
       " '点',\n",
       " '电脑',\n",
       " '电视',\n",
       " '电影',\n",
       " '东西',\n",
       " '读',\n",
       " '对',\n",
       " '多',\n",
       " '多少',\n",
       " '儿子',\n",
       " '饭店',\n",
       " '飞机',\n",
       " '高兴',\n",
       " '工作',\n",
       " '狗',\n",
       " '汉语',\n",
       " '好',\n",
       " '号',\n",
       " '喝',\n",
       " '很',\n",
       " '后面',\n",
       " '回',\n",
       " '会',\n",
       " '家',\n",
       " '叫',\n",
       " '今天',\n",
       " '开',\n",
       " '看',\n",
       " '看见',\n",
       " '块',\n",
       " '来',\n",
       " '老师',\n",
       " '冷',\n",
       " '里',\n",
       " '妈妈',\n",
       " '吗',\n",
       " '买',\n",
       " '猫',\n",
       " '没',\n",
       " '没关系',\n",
       " '米饭',\n",
       " '明天',\n",
       " '名字',\n",
       " '哪',\n",
       " '哪儿',\n",
       " '那',\n",
       " '呢',\n",
       " '能',\n",
       " '你',\n",
       " '年',\n",
       " '女儿',\n",
       " '朋友',\n",
       " '漂亮',\n",
       " '苹果',\n",
       " '钱',\n",
       " '前面',\n",
       " '请',\n",
       " '去',\n",
       " '热',\n",
       " '人',\n",
       " '认识',\n",
       " '商店',\n",
       " '上',\n",
       " '上午',\n",
       " '少',\n",
       " '谁',\n",
       " '什么',\n",
       " '时候',\n",
       " '是',\n",
       " '书',\n",
       " '水',\n",
       " '水果',\n",
       " '睡觉',\n",
       " '说',\n",
       " '岁',\n",
       " '他',\n",
       " '她',\n",
       " '太',\n",
       " '天气',\n",
       " '听',\n",
       " '同学',\n",
       " '喂',\n",
       " '我',\n",
       " '我们',\n",
       " '下',\n",
       " '下午',\n",
       " '下雨',\n",
       " '先生',\n",
       " '现在',\n",
       " '想',\n",
       " '小',\n",
       " '小姐',\n",
       " '些',\n",
       " '写',\n",
       " '谢谢',\n",
       " '星期',\n",
       " '学生',\n",
       " '学习',\n",
       " '学校',\n",
       " '一点儿',\n",
       " '衣服',\n",
       " '医生',\n",
       " '医院',\n",
       " '椅子',\n",
       " '有',\n",
       " '月',\n",
       " '在',\n",
       " '怎么',\n",
       " '怎么样',\n",
       " '这',\n",
       " '中国',\n",
       " '中午',\n",
       " '住',\n",
       " '桌子',\n",
       " '字',\n",
       " '昨天',\n",
       " '做',\n",
       " '坐']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_hsk_vocabulary(HSK_1_VOCABULARY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 151 words from ../../datasets/vocabulary/hsk1.txt\n",
      "Processed 18896 sentences\n",
      "Found 26 sentences containing only HSK vocabulary characters\n",
      "Successfully loaded 151 words from ../../datasets/vocabulary/hsk1.txt\n",
      "Successfully loaded 308 words from ../../datasets/vocabulary/hsk2.txt\n",
      "Successfully loaded 686 words from ../../datasets/vocabulary/hsk3.txt\n",
      "\n",
      "Processing dataset: ../../datasets/raw/hsk_sentences.txt\n",
      "\n",
      "Filtering for hsk1...\n",
      "Processed 18896 sentences\n",
      "Found 26 sentences containing only HSK vocabulary characters\n",
      "Saved 26 sentences to ../../datasets/hsk_sentences/hsk1.txt\n",
      "\n",
      "Filtering for hsk2...\n",
      "Processed 18896 sentences\n",
      "Found 207 sentences containing only HSK vocabulary characters\n",
      "Saved 207 sentences to ../../datasets/hsk_sentences/hsk2.txt\n",
      "\n",
      "Filtering for hsk3...\n",
      "Processed 18896 sentences\n",
      "Found 924 sentences containing only HSK vocabulary characters\n",
      "Saved 924 sentences to ../../datasets/hsk_sentences/hsk3.txt\n",
      "\n",
      "Processing dataset: ../../datasets/raw/cmn_sentences.txt\n",
      "\n",
      "Filtering for hsk1...\n",
      "Processed 72171 sentences\n",
      "Found 1847 sentences containing only HSK vocabulary characters\n",
      "Saved 1847 sentences to ../../datasets/hsk_sentences/hsk1.txt\n",
      "\n",
      "Filtering for hsk2...\n",
      "Processed 72171 sentences\n",
      "Found 6913 sentences containing only HSK vocabulary characters\n",
      "Saved 6913 sentences to ../../datasets/hsk_sentences/hsk2.txt\n",
      "\n",
      "Filtering for hsk3...\n",
      "Processed 72171 sentences\n",
      "Found 14405 sentences containing only HSK vocabulary characters\n",
      "Saved 14405 sentences to ../../datasets/hsk_sentences/hsk3.txt\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH_LIST = [\n",
    "    \"../../datasets/raw/hsk_sentences.txt\",\n",
    "    \"../../datasets/raw/cmn_sentences.txt\"\n",
    "]\n",
    "\n",
    "HSK_1_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk1.txt\"\n",
    "HSK_2_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk2.txt\"\n",
    "HSK_3_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk3.txt\"\n",
    "\n",
    "TARGET_FOLDER = \"../../datasets/hsk_sentences\"\n",
    "\n",
    "import os\n",
    "\n",
    "hsk_words = read_hsk_vocabulary(HSK_1_VOCABULARY_PATH)\n",
    "filtered_sentences = filter_sentences_by_hsk_vocabulary(hsk_words, \"../../datasets/raw/hsk_sentences.txt\")\n",
    "\n",
    "def collect_hsk_sentences(dataset_paths, hsk_paths, target_folder):\n",
    "    \"\"\"\n",
    "    Collect sentences for each HSK level from multiple datasets and save them to separate files.\n",
    "    \n",
    "    Args:\n",
    "        dataset_paths (list): List of paths to dataset files\n",
    "        hsk_paths (dict): Dictionary mapping HSK levels to their vocabulary file paths\n",
    "        target_folder (str): Path to save the filtered sentences\n",
    "    \"\"\"\n",
    "    # Create target folder if it doesn't exist\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    \n",
    "    # Load HSK vocabularies\n",
    "    hsk_vocabularies = {}\n",
    "    for level, path in hsk_paths.items():\n",
    "        hsk_vocabularies[level] = read_hsk_vocabulary(path)\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_path in dataset_paths:\n",
    "        print(f\"\\nProcessing dataset: {dataset_path}\")\n",
    "        \n",
    "        # Filter sentences for each HSK level\n",
    "        for level, vocabulary in hsk_vocabularies.items():\n",
    "            print(f\"\\nFiltering for {level}...\")\n",
    "            filtered_sentences = filter_sentences_by_hsk_vocabulary(vocabulary, dataset_path)\n",
    "            \n",
    "            # Save filtered sentences\n",
    "            output_path = os.path.join(target_folder, f\"{level}.txt\")\n",
    "            with open(output_path, 'a', encoding='utf-8') as f:\n",
    "                for sentence in filtered_sentences:\n",
    "                    f.write(sentence + '\\n')\n",
    "            \n",
    "            print(f\"Saved {len(filtered_sentences)} sentences to {output_path}\")\n",
    "\n",
    "# Define HSK paths dictionary\n",
    "hsk_paths = {\n",
    "    'hsk1': HSK_1_VOCABULARY_PATH,\n",
    "    'hsk2': HSK_2_VOCABULARY_PATH,\n",
    "    'hsk3': HSK_3_VOCABULARY_PATH\n",
    "}\n",
    "\n",
    "# Run the collection process\n",
    "collect_hsk_sentences(DATASET_PATH_LIST, hsk_paths, TARGET_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset: ../../datasets/raw/hsk_sentences.txt\n",
      "\n",
      "Filtering for hsk1...\n",
      "Saved 18440 sentences to ../../train_datasets/hsk1.json\n",
      "\n",
      "Filtering for hsk2...\n",
      "Saved 18688 sentences to ../../train_datasets/hsk2.json\n",
      "\n",
      "Filtering for hsk3...\n",
      "Saved 18809 sentences to ../../train_datasets/hsk3.json\n",
      "\n",
      "Processing dataset: ../../datasets/raw/cmn_sentences.txt\n",
      "\n",
      "Filtering for hsk1...\n",
      "Saved 70279 sentences to ../../train_datasets/hsk1.json\n",
      "\n",
      "Filtering for hsk2...\n",
      "Saved 71145 sentences to ../../train_datasets/hsk2.json\n",
      "\n",
      "Filtering for hsk3...\n",
      "Saved 71508 sentences to ../../train_datasets/hsk3.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "HSK_1_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk1.txt\"\n",
    "HSK_2_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk2.txt\"\n",
    "HSK_3_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk3.txt\"\n",
    "\n",
    "DATASET_PATH_LIST = [\n",
    "    \"../../datasets/raw/hsk_sentences.txt\",\n",
    "    \"../../datasets/raw/cmn_sentences.txt\"\n",
    "]\n",
    "\n",
    "def collect_hsk_sentences_with_labels(dataset_paths, hsk_paths, target_folder):\n",
    "    \"\"\"\n",
    "    Collect sentences for each HSK level and create JSON files with labeled sentences.\n",
    "    \n",
    "    Args:\n",
    "        dataset_paths (list): List of paths to dataset files\n",
    "        hsk_paths (dict): Dictionary mapping HSK levels to their vocabulary file paths\n",
    "        target_folder (str): Path to save the JSON files\n",
    "    \"\"\"\n",
    "    # Create target folder if it doesn't exist\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    \n",
    "    # Load HSK vocabularies\n",
    "    hsk_vocabularies = {}\n",
    "    for level, path in hsk_paths.items():\n",
    "        hsk_vocabularies[level] = read_hsk_vocabulary(path)\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_path in dataset_paths:\n",
    "        print(f\"\\nProcessing dataset: {dataset_path}\")\n",
    "        \n",
    "        # Filter sentences for each HSK level\n",
    "        for level, vocabulary in hsk_vocabularies.items():\n",
    "            print(f\"\\nFiltering for {level}...\")\n",
    "            filtered_sentences = filter_sentences_by_hsk_vocabulary(vocabulary, dataset_path)\n",
    "            \n",
    "            # Create JSON data\n",
    "            json_data = []\n",
    "            for sentence in filtered_sentences:\n",
    "                # Find all HSK words in the sentence\n",
    "                hsk_words_in_sentence = []\n",
    "                for word in vocabulary:\n",
    "                    if word in sentence:\n",
    "                        hsk_words_in_sentence.append(word)\n",
    "                \n",
    "                # Create labeled sentence\n",
    "                labeled_sentence = \"\"\n",
    "                if hsk_words_in_sentence:\n",
    "                    labeled_sentence = f\"输入词语：{hsk_words_in_sentence[0]}。生成句子：{sentence}\"\n",
    "                \n",
    "                # Create sentence object\n",
    "                sentence_obj = {\n",
    "                    \"hsk\": int(level[-1]),  # Extract number from 'hsk1', 'hsk2', etc.\n",
    "                    \"original_sentence\": sentence,\n",
    "                    \"labeled_sentence\": labeled_sentence\n",
    "                }\n",
    "                json_data.append(sentence_obj)\n",
    "            \n",
    "            # Save to JSON file\n",
    "            output_path = os.path.join(target_folder, f\"{level}.json\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"Saved {len(json_data)} sentences to {output_path}\")\n",
    "\n",
    "# Define HSK paths dictionary\n",
    "hsk_paths = {\n",
    "    'hsk1': HSK_1_VOCABULARY_PATH,\n",
    "    'hsk2': HSK_2_VOCABULARY_PATH,\n",
    "    'hsk3': HSK_3_VOCABULARY_PATH\n",
    "}\n",
    "\n",
    "TARGET_FOLDER = \"../../train_datasets/\"\n",
    "\n",
    "# Run the collection process\n",
    "collect_hsk_sentences_with_labels(DATASET_PATH_LIST, hsk_paths, TARGET_FOLDER)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
