{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jieba in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (0.42.1)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/danilkladnitsky/.pyenv/versions/3.10.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "HSK_SENTENCES_PATH = \"../../datasets/hsk_sentences/\"\n",
    "HSK_LEVELS = [\"hsk1\", \"hsk2\", \"hsk3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import os\n",
    "\n",
    "LIMIT_MAIN_WORDS = 4\n",
    "MIN_MAIN_WORD_LENGTH = 1\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    return jieba.lcut(sentence)\n",
    "\n",
    "def extract_main_words(sentence):\n",
    "    sentence = sentence.strip().replace(\"\\n\", \"\")  # ✅ remove line breaks\n",
    "\n",
    "    tokens = tokenize_sentence(sentence)\n",
    "\n",
    "    filtered = [\n",
    "        word for word in tokens\n",
    "        if len(word) >= MIN_MAIN_WORD_LENGTH and not re.match(r\"[。！？…，：；《》、（）“”‘’]\", word)\n",
    "    ]\n",
    "\n",
    "    return filtered[:LIMIT_MAIN_WORDS]\n",
    "\n",
    "def create_prompt(word, sentence):\n",
    "    return f\"为词语“{word}”造句：{sentence}\"\n",
    "\n",
    "def label_hsk_dataset(dataset_path, target_folder):\n",
    "    result = []\n",
    "    # Read dataset\n",
    "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
    "        sentences = f.readlines()\n",
    "\n",
    "    # For each sentence, extract main words and generate labeled sentence\n",
    "    for sentence in sentences:\n",
    "        main_words = extract_main_words(sentence)\n",
    "\n",
    "        current_word_index = 0\n",
    "        for word in main_words:\n",
    "            if current_word_index >= LIMIT_MAIN_WORDS:\n",
    "                break\n",
    "            prompt = create_prompt(word, sentence)\n",
    "            result.append(prompt)\n",
    "            current_word_index += 1\n",
    "\n",
    "    # Ensure target folder exists\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "    \n",
    "    # Define output file path\n",
    "    output_path = os.path.join(target_folder, \"labeled_dataset.txt\")\n",
    "    \n",
    "    # Write result to file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for line in result:\n",
    "            f.write(line.strip() + \"\\n\")\n",
    "\n",
    "    return result\n",
    "\n",
    "labeled_dataset = label_hsk_dataset(\"../../datasets/hsk_sentences/hsk1.txt\", \"../../datasets/for_train/hsk1/\")\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "HSK_1_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk1.txt\"\n",
    "HSK_2_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk2.txt\"\n",
    "HSK_3_VOCABULARY_PATH = \"../../datasets/vocabulary/hsk3.txt\"\n",
    "\n",
    "DATASET_PATH_LIST = [\n",
    "    \"../../datasets/raw/hsk_sentences.txt\",\n",
    "    \"../../datasets/raw/cmn_sentences.txt\"\n",
    "]\n",
    "\n",
    "def collect_hsk_sentences_with_labels(dataset_paths, hsk_paths, target_folder):\n",
    "    \"\"\"\n",
    "    Collect sentences for each HSK level and create JSON files with labeled sentences.\n",
    "    \n",
    "    Args:\n",
    "        dataset_paths (list): List of paths to dataset files\n",
    "        hsk_paths (dict): Dictionary mapping HSK levels to their vocabulary file paths\n",
    "        target_folder (str): Path to save the JSON files\n",
    "    \"\"\"\n",
    "    # Create target folder if it doesn't exist\n",
    "    os.makedirs(target_folder, exist_ok=True)\n",
    "\n",
    "    PROCESS_EACH_HSK_WORD = False\n",
    "    \n",
    "    # Load HSK vocabularies\n",
    "    hsk_vocabularies = {}\n",
    "    for level, path in hsk_paths.items():\n",
    "        hsk_vocabularies[level] = read_hsk_vocabulary(path)\n",
    "    \n",
    "    # Process each dataset\n",
    "    for dataset_path in dataset_paths:\n",
    "        print(f\"\\nProcessing dataset: {dataset_path}\")\n",
    "        \n",
    "        # Filter sentences for each HSK level\n",
    "        for level, vocabulary in hsk_vocabularies.items():\n",
    "            print(f\"\\nFiltering for {level}...\")\n",
    "            filtered_sentences = filter_sentences_by_hsk_vocabulary(vocabulary, dataset_path)\n",
    "            \n",
    "            # Create JSON data\n",
    "            json_data = []\n",
    "            for sentence in filtered_sentences:\n",
    "                # Find all HSK words in the sentence\n",
    "                hsk_words_in_sentence = []\n",
    "                for word in vocabulary:\n",
    "                    if word in sentence:\n",
    "                        hsk_words_in_sentence.append(word)\n",
    "                \n",
    "                if PROCESS_EACH_HSK_WORD:\n",
    "                    for word in hsk_words_in_sentence:\n",
    "                        # Create labeled sentence\n",
    "                        labeled_sentence = \"\"\n",
    "                        if hsk_words_in_sentence:\n",
    "                            labeled_sentence = f\"为词语“{word}”造句：{sentence}\"\n",
    "                        \n",
    "                        # Create sentence object\n",
    "                        sentence_obj = {\n",
    "                            \"hsk\": int(level[-1]),  # Extract number from 'hsk1', 'hsk2', etc.\n",
    "                            \"original_sentence\": sentence,\n",
    "                            \"labeled_sentence\": labeled_sentence\n",
    "                        }\n",
    "                        json_data.append(sentence_obj)\n",
    "                else:\n",
    "                    # Create labeled sentence\n",
    "                    labeled_sentence = \"\"\n",
    "                    if hsk_words_in_sentence:\n",
    "                        labeled_sentence = f\"为词语“{hsk_words_in_sentence[0]}”造句：{sentence}\"\n",
    "                    \n",
    "                    # Create sentence object\n",
    "            \n",
    "            # Save to JSON file\n",
    "            output_path = os.path.join(target_folder, f\"{level}.json\")\n",
    "            with open(output_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(json_data, f, ensure_ascii=False, indent=2)\n",
    "            \n",
    "            print(f\"Saved {len(json_data)} sentences to {output_path}\")\n",
    "\n",
    "# Define HSK paths dictionary\n",
    "hsk_paths = {\n",
    "    'hsk1': HSK_1_VOCABULARY_PATH,\n",
    "    'hsk2': HSK_2_VOCABULARY_PATH,\n",
    "    'hsk3': HSK_3_VOCABULARY_PATH\n",
    "}\n",
    "\n",
    "TARGET_FOLDER = \"../../train_datasets/\"\n",
    "\n",
    "# Run the collection process\n",
    "collect_hsk_sentences_with_labels(DATASET_PATH_LIST, hsk_paths, TARGET_FOLDER)\n",
    "\n",
    "def extract_labeled_sentences(json_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract labeled sentences from a JSON file and save them to a text file.\n",
    "    \n",
    "    Args:\n",
    "        json_path (str): Path to the input JSON file\n",
    "        output_path (str): Path to save the output text file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract labeled sentences\n",
    "        labeled_sentences = [item['labeled_sentence'] for item in data if item['labeled_sentence']]\n",
    "        \n",
    "        # Write to output file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for sentence in labeled_sentences:\n",
    "                f.write(sentence + '\\n')\n",
    "        \n",
    "        print(f\"Successfully extracted {len(labeled_sentences)} labeled sentences to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "TARGET_DATASET_PATH = \"../../train_datasets/json/hsk1.json\"\n",
    "OUTPUT_PATH = \"../../train_datasets/txt/hsk1_labeled.txt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# Extract labeled sentences\n",
    "extract_labeled_sentences(TARGET_DATASET_PATH, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
