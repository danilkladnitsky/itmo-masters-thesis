{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: requests in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: fsspec in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/danilkladnitsky/.pyenv/versions/3.10.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_models(models, word=\"喜欢\", max_length=60, num_return_sequences=3):\n",
    "    from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "    import torch\n",
    "    import re\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for model_path in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "\n",
    "        prompt = f\"请用词语“{word}”造句：\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                temperature=0.4,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        decoded_outputs = []\n",
    "        keyword_count = 0\n",
    "        lengths = []\n",
    "\n",
    "        for output in outputs:\n",
    "            decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            sentence = decoded.replace(prompt, \"\").replace(\" \", \"\").strip()\n",
    "            sentence = re.split(r\"[。！？]\", sentence)[0] + \"。\"\n",
    "            decoded_outputs.append(sentence)\n",
    "            if word in sentence:\n",
    "                keyword_count += 1\n",
    "            lengths.append(len(sentence))\n",
    "\n",
    "        results[model_path] = {\n",
    "            \"samples\": decoded_outputs,\n",
    "            \"keyword_coverage\": keyword_count / num_return_sequences,\n",
    "            \"avg_length\": sum(lengths) / len(lengths),\n",
    "            \"inference_time_sec\": elapsed,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'../models/grid_search/run_0/checkpoint-312': {'avg_length': 15.666666666666666,\n",
      "                                                            'inference_time_sec': 0.831632137298584,\n",
      "                                                            'keyword_coverage': 1.0,\n",
      "                                                            'samples': ['请用词语“喜欢”造句:她是我的。',\n",
      "                                                                        '请用词语“喜欢”造句:她有谁。',\n",
      "                                                                        '请用词语“喜欢”造句:你是我的。']},\n",
      "             '../models/grid_search/run_1/checkpoint-312': {'avg_length': 17.333333333333332,\n",
      "                                                            'inference_time_sec': 0.8303139209747314,\n",
      "                                                            'keyword_coverage': 1.0,\n",
      "                                                            'samples': ['请用词语“喜欢”造句:我高兴了。',\n",
      "                                                                        '请用词语“喜欢”造句:今天是我的朋友。',\n",
      "                                                                        '请用词语“喜欢”造句:我高兴在在。']},\n",
      "             '../models/grid_search/run_2/checkpoint-312': {'avg_length': 18.333333333333332,\n",
      "                                                            'inference_time_sec': 1.5066499710083008,\n",
      "                                                            'keyword_coverage': 1.0,\n",
      "                                                            'samples': ['请用词语“喜欢”造句:我你我没有。',\n",
      "                                                                        '请用词语“喜欢”造句:我”造句:我是现在。',\n",
      "                                                                        '请用词语“喜欢”造句:我你我没有。']}})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "models = [\n",
    "    \"../models/grid_search/run_0/checkpoint-312\",\n",
    "    \"../models/grid_search/run_1/checkpoint-312\",\n",
    "    \"../models/grid_search/run_2/checkpoint-312\",\n",
    "]\n",
    "\n",
    "comparison = compare_models(models, word=\"喜欢\", num_return_sequences=3)\n",
    "pprint(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_perplexity(model, tokenizer, sentences: list, max_length: int = 512) -> float:\n",
    "    \"\"\"\n",
    "    Compute perplexity of a language model on a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        model: A language model (e.g., GPT2LMHeadModel)\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        sentences (list): A list of strings to evaluate\n",
    "        max_length (int): Maximum length to truncate input to\n",
    "    \n",
    "    Returns:\n",
    "        float: Perplexity score (lower is better)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = output.loss\n",
    "\n",
    "        num_tokens = inputs[\"input_ids\"].numel()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\")  # avoid division by zero\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: ../models/grid_search/run_0/checkpoint-312\n",
      "🔍 Perplexity for ../models/grid_search/run_0/checkpoint-312: nan\n",
      "\n",
      "Evaluating: ../models/grid_search/run_1/checkpoint-312\n",
      "🔍 Perplexity for ../models/grid_search/run_1/checkpoint-312: nan\n",
      "\n",
      "Evaluating: ../models/grid_search/run_2/checkpoint-312\n",
      "🔍 Perplexity for ../models/grid_search/run_2/checkpoint-312: nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "def compute_perplexity(model, tokenizer, sentences: list, max_length: int = 512) -> float:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        if inputs[\"input_ids\"].numel() == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = output.loss\n",
    "\n",
    "        num_tokens = inputs[\"input_ids\"].numel()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        print(\"⚠️ No tokens were processed. All inputs may be empty or not tokenizable by this model.\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity\n",
    "# Paths\n",
    "models = [\n",
    "    \"../models/grid_search/run_0/checkpoint-312\",\n",
    "    \"../models/grid_search/run_1/checkpoint-312\",\n",
    "    \"../models/grid_search/run_2/checkpoint-312\"\n",
    "]\n",
    "\n",
    "hsk_dataset_path_json = \"../datasets/hsk1-dataset.json\"\n",
    "\n",
    "# Load evaluation sentences\n",
    "# Load line-delimited JSON (NDJSON-style)\n",
    "with open(hsk_dataset_path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Expecting format: [{ \"prompt\": ..., \"completion\": ... }]\n",
    "eval_sentences = [entry[\"completion\"].strip() for entry in data if \"completion\" in entry]\n",
    "\n",
    "# Evaluate perplexity per model\n",
    "for model_path in models:\n",
    "    print(f\"Evaluating: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ppl = compute_perplexity(model, tokenizer, eval_sentences)\n",
    "    print(f\"🔍 Perplexity for {model_path}: {ppl:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "def calculate_hsk_coverage(sentences: list, target_word: str, hsk_vocab: set) -> dict:\n",
    "    \"\"\"\n",
    "    Check HSK word coverage and target word presence in generated Chinese sentences.\n",
    "    Uses jieba for tokenization and ignores punctuation.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of generated Chinese sentences.\n",
    "        target_word (str): The word that must appear in each sentence.\n",
    "        hsk_vocab (set): Set of allowed HSK words (characters or full words).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary statistics and details per sentence.\n",
    "    \"\"\"\n",
    "    punctuation_pattern = r\"[，。！？、,.!?；;：“”\\\"'（）()【】\\[\\]《》<>]\"\n",
    "\n",
    "    results = []\n",
    "    in_vocab_sentences = 0\n",
    "    target_present_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip().replace(\" \", \"\")\n",
    "        sentence_clean = re.sub(punctuation_pattern, \"\", sentence)\n",
    "\n",
    "        tokens = list(jieba.cut(sentence_clean))\n",
    "        in_vocab = all(token in hsk_vocab for token in tokens)\n",
    "        contains_target = target_word in sentence_clean\n",
    "\n",
    "        if in_vocab:\n",
    "            in_vocab_sentences += 1\n",
    "        if contains_target:\n",
    "            target_present_count += 1\n",
    "\n",
    "        results.append({\n",
    "            \"original_sentence\": sentence,\n",
    "            \"cleaned_sentence\": sentence_clean,\n",
    "            \"tokens\": tokens,\n",
    "            \"all_in_vocab\": in_vocab,\n",
    "            \"contains_target\": contains_target,\n",
    "            \"unknown_tokens\": [t for t in tokens if t not in hsk_vocab]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"total\": len(sentences),\n",
    "        \"target_word\": target_word,\n",
    "        \"target_word_coverage\": target_present_count / len(sentences),\n",
    "        \"full_vocab_coverage\": in_vocab_sentences / len(sentences),\n",
    "        \"details\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 HSK words.\n",
      "['妈妈', '叫', '出租车', '前面', '的', '住', '几', '本', '名字', '吗']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calculate_hsk_coverage() got an unexpected keyword argument 'target_word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(hsk_vocab)[:\u001b[38;5;241m10\u001b[39m])  \u001b[38;5;66;03m# Show first 10 entries\u001b[39;00m\n\u001b[1;32m     22\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m我非常喜欢吃苹果。\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m他喜欢看电影。\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m我爱吃披萨。\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m ]\n\u001b[0;32m---> 31\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_hsk_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m喜欢\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhsk_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhsk_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m pprint(report)\n",
      "\u001b[0;31mTypeError\u001b[0m: calculate_hsk_coverage() got an unexpected keyword argument 'target_word'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def load_hsk_vocab(filepath: str) -> set:\n",
    "    \"\"\"\n",
    "    Load HSK vocabulary from a file and return it as a set of words.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the file containing HSK words (one per line).\n",
    "    \n",
    "    Returns:\n",
    "        set: Set of unique HSK words or characters.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = {line.strip() for line in f if line.strip()}\n",
    "    return vocab\n",
    "\n",
    "hsk_vocab = load_hsk_vocab(\"../datasets/vocabulary/hsk1.txt\")\n",
    "\n",
    "print(f\"Loaded {len(hsk_vocab)} HSK words.\")\n",
    "print(list(hsk_vocab)[:10])  # Show first 10 entries\n",
    "\n",
    "sentences = [\n",
    "    \"我非常喜欢吃苹果。\",\n",
    "    \"他喜欢看电影。\",\n",
    "    \"小明喜欢踢足球。\",\n",
    "    \"老师喜欢认真听讲的学生。\",\n",
    "    \"她喜欢安静的环境。\",\n",
    "    \"我爱吃披萨。\"\n",
    "]\n",
    "\n",
    "report = calculate_hsk_coverage(sentences, target_word=\"喜欢\", hsk_vocab=hsk_vocab)\n",
    "\n",
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import torch\n",
    "import re\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub(r\"[，。！？、,.!?；;：“”\\\"'（）()【】\\[\\]《》<>]\", \"\", text)\n",
    "\n",
    "def evaluate_models_on_hsk(models: list, hsk_vocab: set, max_length: int = 50) -> list:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    skip_tokens = {\"用\", \"词语\", \"造句\", \"请\", \":\"}\n",
    "    summaries = []\n",
    "\n",
    "    for model_path in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path).to(device).eval()\n",
    "\n",
    "        generated_sentences = []\n",
    "        hsk_words = list(hsk_vocab)\n",
    "        target_words = []\n",
    "\n",
    "        for word in hsk_words:\n",
    "            prompt = f\"请用词语“{word}”造句：\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.9,\n",
    "                    temperature=0.7,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            sentence = decoded.replace(prompt, \"\").replace(\" \", \"\").strip()\n",
    "            sentence = re.split(r\"[。！？]\", sentence)[0] + \"。\"  # stop at first major punctuation\n",
    "            generated_sentences.append(sentence)\n",
    "            target_words.append(word)\n",
    "\n",
    "        # Calculate coverage\n",
    "        total = len(generated_sentences)\n",
    "        hsk_compliant_count = 0\n",
    "        target_hit_count = 0\n",
    "        total_unknown_tokens = 0\n",
    "        unknown_token_set = set()\n",
    "\n",
    "        for sentence, target_word in zip(generated_sentences, target_words):\n",
    "            cleaned = remove_punctuation(sentence).replace(\" \", \"\")\n",
    "            cleaned = ''.join([ch for ch in cleaned if ch not in skip_tokens])\n",
    "\n",
    "            tokens = list(jieba.cut(cleaned))\n",
    "            unknown_tokens = [t for t in tokens if any(c not in hsk_vocab for c in t)]\n",
    "\n",
    "            # ✅ Check if all characters (from all tokens) are in HSK vocab\n",
    "            all_chars = [char for token in tokens for char in token]\n",
    "            if all(char in hsk_vocab for char in all_chars):\n",
    "                hsk_compliant_count += 1\n",
    "\n",
    "            if target_word in cleaned:\n",
    "                target_hit_count += 1\n",
    "\n",
    "            total_unknown_tokens += len(unknown_tokens)\n",
    "            unknown_token_set.update(unknown_tokens)\n",
    "\n",
    "        summaries.append({\n",
    "            \"model_path\": model_path,\n",
    "            \"target_word_coverage\": target_hit_count / total,\n",
    "            \"full_vocab_coverage\": hsk_compliant_count / total,\n",
    "            \"avg_unknown_tokens_per_sentence\": total_unknown_tokens / total,\n",
    "            \"unknown_tokens\": sorted(list(unknown_token_set))\n",
    "        })\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'avg_unknown_tokens_per_sentence': 3.3311258278145695,\n",
      "  'full_vocab_coverage': 0.0,\n",
      "  'model_path': '../models/grid_search/run_0/checkpoint-312',\n",
      "  'target_word_coverage': 0.9867549668874173,\n",
      "  'unknown_tokens': ['一点儿',\n",
      "                     '一起',\n",
      "                     '上午',\n",
      "                     '下午',\n",
      "                     '下雨',\n",
      "                     '东西',\n",
      "                     '中午',\n",
      "                     '中国',\n",
      "                     '为什么',\n",
      "                     '人造',\n",
      "                     '什么',\n",
      "                     '今天',\n",
      "                     '儿子',\n",
      "                     '先生',\n",
      "                     '再见',\n",
      "                     '出去',\n",
      "                     '出租车',\n",
      "                     '分钟',\n",
      "                     '前面',\n",
      "                     '北京',\n",
      "                     '医生',\n",
      "                     '医院',\n",
      "                     '句',\n",
      "                     '吃饭',\n",
      "                     '同学',\n",
      "                     '名字',\n",
      "                     '后面',\n",
      "                     '哪儿',\n",
      "                     '哪里',\n",
      "                     '商店',\n",
      "                     '喜欢',\n",
      "                     '天气',\n",
      "                     '女儿',\n",
      "                     '妈妈',\n",
      "                     '学习',\n",
      "                     '学校',\n",
      "                     '学生',\n",
      "                     '客气',\n",
      "                     '对不起',\n",
      "                     '小姐',\n",
      "                     '工作',\n",
      "                     '店里',\n",
      "                     '怎么',\n",
      "                     '怎么样',\n",
      "                     '我们',\n",
      "                     '打电话',\n",
      "                     '时候',\n",
      "                     '明天',\n",
      "                     '星期',\n",
      "                     '昨天',\n",
      "                     '有时候',\n",
      "                     '朋友',\n",
      "                     '杯子',\n",
      "                     '桌子',\n",
      "                     '椅子',\n",
      "                     '水果',\n",
      "                     '汉语',\n",
      "                     '没关系',\n",
      "                     '没有',\n",
      "                     '漂亮',\n",
      "                     '爸爸',\n",
      "                     '现在',\n",
      "                     '电影',\n",
      "                     '电脑',\n",
      "                     '电视',\n",
      "                     '看见',\n",
      "                     '睡觉',\n",
      "                     '米饭',\n",
      "                     '老师',\n",
      "                     '苹果',\n",
      "                     '衣服',\n",
      "                     '见',\n",
      "                     '认为',\n",
      "                     '认识',\n",
      "                     '词语',\n",
      "                     '谢谢',\n",
      "                     '车车',\n",
      "                     '造句',\n",
      "                     '里面',\n",
      "                     '飞机',\n",
      "                     '饭店',\n",
      "                     '高兴']},\n",
      " {'avg_unknown_tokens_per_sentence': 3.662251655629139,\n",
      "  'full_vocab_coverage': 0.0,\n",
      "  'model_path': '../models/grid_search/run_1/checkpoint-312',\n",
      "  'target_word_coverage': 0.9867549668874173,\n",
      "  'unknown_tokens': ['一点儿',\n",
      "                     '一起',\n",
      "                     '上午',\n",
      "                     '下午',\n",
      "                     '下雨',\n",
      "                     '东西',\n",
      "                     '中午',\n",
      "                     '中国',\n",
      "                     '人造',\n",
      "                     '什么',\n",
      "                     '今天',\n",
      "                     '他们',\n",
      "                     '你们',\n",
      "                     '儿子',\n",
      "                     '先生',\n",
      "                     '再见',\n",
      "                     '出租车',\n",
      "                     '分钟',\n",
      "                     '前面',\n",
      "                     '北京',\n",
      "                     '医生',\n",
      "                     '医院',\n",
      "                     '句',\n",
      "                     '同学',\n",
      "                     '名字',\n",
      "                     '后面',\n",
      "                     '哪儿',\n",
      "                     '哪里',\n",
      "                     '商店',\n",
      "                     '喜欢',\n",
      "                     '天气',\n",
      "                     '女儿',\n",
      "                     '妈妈',\n",
      "                     '学习',\n",
      "                     '学校',\n",
      "                     '学生',\n",
      "                     '客气',\n",
      "                     '对',\n",
      "                     '对不起',\n",
      "                     '小姐',\n",
      "                     '工作',\n",
      "                     '怎么',\n",
      "                     '怎么样',\n",
      "                     '我们',\n",
      "                     '打电话',\n",
      "                     '时候',\n",
      "                     '明天',\n",
      "                     '星期',\n",
      "                     '昨天',\n",
      "                     '朋友',\n",
      "                     '杯子',\n",
      "                     '桌子',\n",
      "                     '椅子',\n",
      "                     '水果',\n",
      "                     '汉语',\n",
      "                     '没',\n",
      "                     '没关系',\n",
      "                     '没有',\n",
      "                     '没狗',\n",
      "                     '漂亮',\n",
      "                     '爸爸',\n",
      "                     '现在',\n",
      "                     '电影',\n",
      "                     '电脑',\n",
      "                     '电视',\n",
      "                     '看见',\n",
      "                     '睡觉',\n",
      "                     '米饭',\n",
      "                     '老',\n",
      "                     '老师',\n",
      "                     '苹果',\n",
      "                     '衣服',\n",
      "                     '认识',\n",
      "                     '词语',\n",
      "                     '谢谢',\n",
      "                     '车',\n",
      "                     '造句',\n",
      "                     '里面',\n",
      "                     '飞机',\n",
      "                     '饭店',\n",
      "                     '高兴']},\n",
      " {'avg_unknown_tokens_per_sentence': 3.748344370860927,\n",
      "  'full_vocab_coverage': 0.0,\n",
      "  'model_path': '../models/grid_search/run_2/checkpoint-312',\n",
      "  'target_word_coverage': 0.9867549668874173,\n",
      "  'unknown_tokens': ['一点儿',\n",
      "                     '三杯水',\n",
      "                     '上午',\n",
      "                     '下午',\n",
      "                     '下雨',\n",
      "                     '东西',\n",
      "                     '中午',\n",
      "                     '中国',\n",
      "                     '为什么',\n",
      "                     '人造',\n",
      "                     '什么',\n",
      "                     '今天',\n",
      "                     '他们',\n",
      "                     '你们',\n",
      "                     '儿子',\n",
      "                     '先生',\n",
      "                     '再见',\n",
      "                     '出去',\n",
      "                     '出租车',\n",
      "                     '分钟',\n",
      "                     '前面',\n",
      "                     '北京',\n",
      "                     '医生',\n",
      "                     '医院',\n",
      "                     '句',\n",
      "                     '吃饭',\n",
      "                     '同学',\n",
      "                     '名字',\n",
      "                     '后面',\n",
      "                     '哪儿',\n",
      "                     '哪里',\n",
      "                     '商店',\n",
      "                     '喜欢',\n",
      "                     '天气',\n",
      "                     '女儿',\n",
      "                     '妈妈',\n",
      "                     '学习',\n",
      "                     '学校',\n",
      "                     '学生',\n",
      "                     '客气',\n",
      "                     '对',\n",
      "                     '对不起',\n",
      "                     '小姐',\n",
      "                     '工作',\n",
      "                     '怎么',\n",
      "                     '怎么样',\n",
      "                     '我们',\n",
      "                     '打电话',\n",
      "                     '时候',\n",
      "                     '明天',\n",
      "                     '星期',\n",
      "                     '星期三',\n",
      "                     '星期天',\n",
      "                     '昨天',\n",
      "                     '朋友',\n",
      "                     '杯子',\n",
      "                     '样子',\n",
      "                     '桌子',\n",
      "                     '椅子',\n",
      "                     '水果',\n",
      "                     '汉语',\n",
      "                     '没',\n",
      "                     '没什么',\n",
      "                     '没关系',\n",
      "                     '没有',\n",
      "                     '漂亮',\n",
      "                     '爸爸',\n",
      "                     '现在',\n",
      "                     '电影',\n",
      "                     '电脑',\n",
      "                     '电视',\n",
      "                     '看见',\n",
      "                     '睡',\n",
      "                     '睡觉',\n",
      "                     '米饭',\n",
      "                     '老',\n",
      "                     '老师',\n",
      "                     '苹果',\n",
      "                     '衣服',\n",
      "                     '见',\n",
      "                     '认识',\n",
      "                     '词语',\n",
      "                     '谢谢',\n",
      "                     '车去',\n",
      "                     '这里',\n",
      "                     '造句',\n",
      "                     '那天',\n",
      "                     '里面',\n",
      "                     '飞机',\n",
      "                     '饭店',\n",
      "                     '高',\n",
      "                     '高兴']}]\n"
     ]
    }
   ],
   "source": [
    "# Load HSK vocab\n",
    "from pprint import pprint\n",
    "\n",
    "hsk_vocab = load_hsk_vocab(\"../datasets/vocabulary/hsk1.txt\")\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    \"../models/grid_search/run_0/checkpoint-312\",\n",
    "    \"../models/grid_search/run_1/checkpoint-312\",\n",
    "    \"../models/grid_search/run_2/checkpoint-312\"\n",
    "]\n",
    "\n",
    "# cropped_set\n",
    "hsk_vocab = hsk_vocab\n",
    "\n",
    "# Evaluate\n",
    "reports = evaluate_models_on_hsk(models, hsk_vocab)\n",
    "\n",
    "pprint(reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
