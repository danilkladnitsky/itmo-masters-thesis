{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: torch in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: requests in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: fsspec in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/danilkladnitsky/.pyenv/versions/3.10.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "def compare_models(models, word=\"å–œæ¬¢\", max_length=60, num_return_sequences=3):\n",
    "    from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "    import torch\n",
    "    import re\n",
    "\n",
    "    results = defaultdict(list)\n",
    "    \n",
    "    for model_path in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path).eval()\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        model.to(device)\n",
    "\n",
    "        prompt = f\"è¯·ç”¨è¯è¯­â€œ{word}â€é€ å¥ï¼š\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_length=max_length,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                top_p=0.9,\n",
    "                temperature=0.4,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        decoded_outputs = []\n",
    "        keyword_count = 0\n",
    "        lengths = []\n",
    "\n",
    "        for output in outputs:\n",
    "            decoded = tokenizer.decode(output, skip_special_tokens=True)\n",
    "            sentence = decoded.replace(prompt, \"\").replace(\" \", \"\").strip()\n",
    "            sentence = re.split(r\"[ã€‚ï¼ï¼Ÿ]\", sentence)[0] + \"ã€‚\"\n",
    "            decoded_outputs.append(sentence)\n",
    "            if word in sentence:\n",
    "                keyword_count += 1\n",
    "            lengths.append(len(sentence))\n",
    "\n",
    "        results[model_path] = {\n",
    "            \"samples\": decoded_outputs,\n",
    "            \"keyword_coverage\": keyword_count / num_return_sequences,\n",
    "            \"avg_length\": sum(lengths) / len(lengths),\n",
    "            \"inference_time_sec\": elapsed,\n",
    "        }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'../models/grid_search/run_0/checkpoint-312': {'avg_length': 15.666666666666666,\n",
      "                                                            'inference_time_sec': 0.831632137298584,\n",
      "                                                            'keyword_coverage': 1.0,\n",
      "                                                            'samples': ['è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:å¥¹æ˜¯æˆ‘çš„ã€‚',\n",
      "                                                                        'è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:å¥¹æœ‰è°ã€‚',\n",
      "                                                                        'è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:ä½ æ˜¯æˆ‘çš„ã€‚']},\n",
      "             '../models/grid_search/run_1/checkpoint-312': {'avg_length': 17.333333333333332,\n",
      "                                                            'inference_time_sec': 0.8303139209747314,\n",
      "                                                            'keyword_coverage': 1.0,\n",
      "                                                            'samples': ['è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:æˆ‘é«˜å…´äº†ã€‚',\n",
      "                                                                        'è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:ä»Šå¤©æ˜¯æˆ‘çš„æœ‹å‹ã€‚',\n",
      "                                                                        'è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:æˆ‘é«˜å…´åœ¨åœ¨ã€‚']},\n",
      "             '../models/grid_search/run_2/checkpoint-312': {'avg_length': 18.333333333333332,\n",
      "                                                            'inference_time_sec': 1.5066499710083008,\n",
      "                                                            'keyword_coverage': 1.0,\n",
      "                                                            'samples': ['è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:æˆ‘ä½ æˆ‘æ²¡æœ‰ã€‚',\n",
      "                                                                        'è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:æˆ‘â€é€ å¥:æˆ‘æ˜¯ç°åœ¨ã€‚',\n",
      "                                                                        'è¯·ç”¨è¯è¯­â€œå–œæ¬¢â€é€ å¥:æˆ‘ä½ æˆ‘æ²¡æœ‰ã€‚']}})\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "models = [\n",
    "    \"../models/grid_search/run_0/checkpoint-312\",\n",
    "    \"../models/grid_search/run_1/checkpoint-312\",\n",
    "    \"../models/grid_search/run_2/checkpoint-312\",\n",
    "]\n",
    "\n",
    "comparison = compare_models(models, word=\"å–œæ¬¢\", num_return_sequences=3)\n",
    "pprint(comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_perplexity(model, tokenizer, sentences: list, max_length: int = 512) -> float:\n",
    "    \"\"\"\n",
    "    Compute perplexity of a language model on a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        model: A language model (e.g., GPT2LMHeadModel)\n",
    "        tokenizer: Corresponding tokenizer\n",
    "        sentences (list): A list of strings to evaluate\n",
    "        max_length (int): Maximum length to truncate input to\n",
    "    \n",
    "    Returns:\n",
    "        float: Perplexity score (lower is better)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = output.loss\n",
    "\n",
    "        num_tokens = inputs[\"input_ids\"].numel()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(\"inf\")  # avoid division by zero\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: ../models/grid_search/run_0/checkpoint-312\n",
      "ğŸ” Perplexity for ../models/grid_search/run_0/checkpoint-312: nan\n",
      "\n",
      "Evaluating: ../models/grid_search/run_1/checkpoint-312\n",
      "ğŸ” Perplexity for ../models/grid_search/run_1/checkpoint-312: nan\n",
      "\n",
      "Evaluating: ../models/grid_search/run_2/checkpoint-312\n",
      "ğŸ” Perplexity for ../models/grid_search/run_2/checkpoint-312: nan\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "def compute_perplexity(model, tokenizer, sentences: list, max_length: int = 512) -> float:\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    skipped = 0\n",
    "\n",
    "    for sent in sentences:\n",
    "        inputs = tokenizer(sent, return_tensors=\"pt\", truncation=True, max_length=max_length)\n",
    "        if inputs[\"input_ids\"].numel() == 0:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = output.loss\n",
    "\n",
    "        num_tokens = inputs[\"input_ids\"].numel()\n",
    "        total_loss += loss.item() * num_tokens\n",
    "        total_tokens += num_tokens\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        print(\"âš ï¸ No tokens were processed. All inputs may be empty or not tokenizable by this model.\")\n",
    "        return float(\"nan\")\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity\n",
    "# Paths\n",
    "models = [\n",
    "    \"../models/grid_search/run_0/checkpoint-312\",\n",
    "    \"../models/grid_search/run_1/checkpoint-312\",\n",
    "    \"../models/grid_search/run_2/checkpoint-312\"\n",
    "]\n",
    "\n",
    "hsk_dataset_path_json = \"../datasets/hsk1-dataset.json\"\n",
    "\n",
    "# Load evaluation sentences\n",
    "# Load line-delimited JSON (NDJSON-style)\n",
    "with open(hsk_dataset_path_json, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "# Expecting format: [{ \"prompt\": ..., \"completion\": ... }]\n",
    "eval_sentences = [entry[\"completion\"].strip() for entry in data if \"completion\" in entry]\n",
    "\n",
    "# Evaluate perplexity per model\n",
    "for model_path in models:\n",
    "    print(f\"Evaluating: {model_path}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ppl = compute_perplexity(model, tokenizer, eval_sentences)\n",
    "    print(f\"ğŸ” Perplexity for {model_path}: {ppl:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "\n",
    "def calculate_hsk_coverage(sentences: list, target_word: str, hsk_vocab: set) -> dict:\n",
    "    \"\"\"\n",
    "    Check HSK word coverage and target word presence in generated Chinese sentences.\n",
    "    Uses jieba for tokenization and ignores punctuation.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of generated Chinese sentences.\n",
    "        target_word (str): The word that must appear in each sentence.\n",
    "        hsk_vocab (set): Set of allowed HSK words (characters or full words).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary statistics and details per sentence.\n",
    "    \"\"\"\n",
    "    punctuation_pattern = r\"[ï¼Œã€‚ï¼ï¼Ÿã€,.!?ï¼›;ï¼šâ€œâ€\\\"'ï¼ˆï¼‰()ã€ã€‘\\[\\]ã€Šã€‹<>]\"\n",
    "\n",
    "    results = []\n",
    "    in_vocab_sentences = 0\n",
    "    target_present_count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.strip().replace(\" \", \"\")\n",
    "        sentence_clean = re.sub(punctuation_pattern, \"\", sentence)\n",
    "\n",
    "        tokens = list(jieba.cut(sentence_clean))\n",
    "        in_vocab = all(token in hsk_vocab for token in tokens)\n",
    "        contains_target = target_word in sentence_clean\n",
    "\n",
    "        if in_vocab:\n",
    "            in_vocab_sentences += 1\n",
    "        if contains_target:\n",
    "            target_present_count += 1\n",
    "\n",
    "        results.append({\n",
    "            \"original_sentence\": sentence,\n",
    "            \"cleaned_sentence\": sentence_clean,\n",
    "            \"tokens\": tokens,\n",
    "            \"all_in_vocab\": in_vocab,\n",
    "            \"contains_target\": contains_target,\n",
    "            \"unknown_tokens\": [t for t in tokens if t not in hsk_vocab]\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"total\": len(sentences),\n",
    "        \"target_word\": target_word,\n",
    "        \"target_word_coverage\": target_present_count / len(sentences),\n",
    "        \"full_vocab_coverage\": in_vocab_sentences / len(sentences),\n",
    "        \"details\": results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 151 HSK words.\n",
      "['å¦ˆå¦ˆ', 'å«', 'å‡ºç§Ÿè½¦', 'å‰é¢', 'çš„', 'ä½', 'å‡ ', 'æœ¬', 'åå­—', 'å—']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calculate_hsk_coverage() got an unexpected keyword argument 'target_word'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(hsk_vocab)[:\u001b[38;5;241m10\u001b[39m])  \u001b[38;5;66;03m# Show first 10 entries\u001b[39;00m\n\u001b[1;32m     22\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæˆ‘éå¸¸å–œæ¬¢åƒè‹¹æœã€‚\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mä»–å–œæ¬¢çœ‹ç”µå½±ã€‚\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mæˆ‘çˆ±åƒæŠ«è¨ã€‚\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m ]\n\u001b[0;32m---> 31\u001b[0m report \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_hsk_coverage\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_word\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43må–œæ¬¢\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhsk_vocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhsk_vocab\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m pprint(report)\n",
      "\u001b[0;31mTypeError\u001b[0m: calculate_hsk_coverage() got an unexpected keyword argument 'target_word'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "def load_hsk_vocab(filepath: str) -> set:\n",
    "    \"\"\"\n",
    "    Load HSK vocabulary from a file and return it as a set of words.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the file containing HSK words (one per line).\n",
    "    \n",
    "    Returns:\n",
    "        set: Set of unique HSK words or characters.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        vocab = {line.strip() for line in f if line.strip()}\n",
    "    return vocab\n",
    "\n",
    "hsk_vocab = load_hsk_vocab(\"../datasets/vocabulary/hsk1.txt\")\n",
    "\n",
    "print(f\"Loaded {len(hsk_vocab)} HSK words.\")\n",
    "print(list(hsk_vocab)[:10])  # Show first 10 entries\n",
    "\n",
    "sentences = [\n",
    "    \"æˆ‘éå¸¸å–œæ¬¢åƒè‹¹æœã€‚\",\n",
    "    \"ä»–å–œæ¬¢çœ‹ç”µå½±ã€‚\",\n",
    "    \"å°æ˜å–œæ¬¢è¸¢è¶³çƒã€‚\",\n",
    "    \"è€å¸ˆå–œæ¬¢è®¤çœŸå¬è®²çš„å­¦ç”Ÿã€‚\",\n",
    "    \"å¥¹å–œæ¬¢å®‰é™çš„ç¯å¢ƒã€‚\",\n",
    "    \"æˆ‘çˆ±åƒæŠ«è¨ã€‚\"\n",
    "]\n",
    "\n",
    "report = calculate_hsk_coverage(sentences, target_word=\"å–œæ¬¢\", hsk_vocab=hsk_vocab)\n",
    "\n",
    "pprint(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import torch\n",
    "import re\n",
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "def remove_punctuation(text: str) -> str:\n",
    "    return re.sub(r\"[ï¼Œã€‚ï¼ï¼Ÿã€,.!?ï¼›;ï¼šâ€œâ€\\\"'ï¼ˆï¼‰()ã€ã€‘\\[\\]ã€Šã€‹<>]\", \"\", text)\n",
    "\n",
    "def evaluate_models_on_hsk(models: list, hsk_vocab: set, max_length: int = 50) -> list:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    skip_tokens = {\"ç”¨\", \"è¯è¯­\", \"é€ å¥\", \"è¯·\", \":\"}\n",
    "    summaries = []\n",
    "\n",
    "    for model_path in models:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path).to(device).eval()\n",
    "\n",
    "        generated_sentences = []\n",
    "        hsk_words = list(hsk_vocab)\n",
    "        target_words = []\n",
    "\n",
    "        for word in hsk_words:\n",
    "            prompt = f\"è¯·ç”¨è¯è¯­â€œ{word}â€é€ å¥ï¼š\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=max_length,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    top_p=0.9,\n",
    "                    temperature=0.7,\n",
    "                    repetition_penalty=1.2,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            sentence = decoded.replace(prompt, \"\").replace(\" \", \"\").strip()\n",
    "            sentence = re.split(r\"[ã€‚ï¼ï¼Ÿ]\", sentence)[0] + \"ã€‚\"  # stop at first major punctuation\n",
    "            generated_sentences.append(sentence)\n",
    "            target_words.append(word)\n",
    "\n",
    "        # Calculate coverage\n",
    "        total = len(generated_sentences)\n",
    "        hsk_compliant_count = 0\n",
    "        target_hit_count = 0\n",
    "        total_unknown_tokens = 0\n",
    "        unknown_token_set = set()\n",
    "\n",
    "        for sentence, target_word in zip(generated_sentences, target_words):\n",
    "            cleaned = remove_punctuation(sentence).replace(\" \", \"\")\n",
    "            cleaned = ''.join([ch for ch in cleaned if ch not in skip_tokens])\n",
    "\n",
    "            tokens = list(jieba.cut(cleaned))\n",
    "            unknown_tokens = [t for t in tokens if any(c not in hsk_vocab for c in t)]\n",
    "\n",
    "            # âœ… Check if all characters (from all tokens) are in HSK vocab\n",
    "            all_chars = [char for token in tokens for char in token]\n",
    "            if all(char in hsk_vocab for char in all_chars):\n",
    "                hsk_compliant_count += 1\n",
    "\n",
    "            if target_word in cleaned:\n",
    "                target_hit_count += 1\n",
    "\n",
    "            total_unknown_tokens += len(unknown_tokens)\n",
    "            unknown_token_set.update(unknown_tokens)\n",
    "\n",
    "        summaries.append({\n",
    "            \"model_path\": model_path,\n",
    "            \"target_word_coverage\": target_hit_count / total,\n",
    "            \"full_vocab_coverage\": hsk_compliant_count / total,\n",
    "            \"avg_unknown_tokens_per_sentence\": total_unknown_tokens / total,\n",
    "            \"unknown_tokens\": sorted(list(unknown_token_set))\n",
    "        })\n",
    "\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'avg_unknown_tokens_per_sentence': 3.3311258278145695,\n",
      "  'full_vocab_coverage': 0.0,\n",
      "  'model_path': '../models/grid_search/run_0/checkpoint-312',\n",
      "  'target_word_coverage': 0.9867549668874173,\n",
      "  'unknown_tokens': ['ä¸€ç‚¹å„¿',\n",
      "                     'ä¸€èµ·',\n",
      "                     'ä¸Šåˆ',\n",
      "                     'ä¸‹åˆ',\n",
      "                     'ä¸‹é›¨',\n",
      "                     'ä¸œè¥¿',\n",
      "                     'ä¸­åˆ',\n",
      "                     'ä¸­å›½',\n",
      "                     'ä¸ºä»€ä¹ˆ',\n",
      "                     'äººé€ ',\n",
      "                     'ä»€ä¹ˆ',\n",
      "                     'ä»Šå¤©',\n",
      "                     'å„¿å­',\n",
      "                     'å…ˆç”Ÿ',\n",
      "                     'å†è§',\n",
      "                     'å‡ºå»',\n",
      "                     'å‡ºç§Ÿè½¦',\n",
      "                     'åˆ†é’Ÿ',\n",
      "                     'å‰é¢',\n",
      "                     'åŒ—äº¬',\n",
      "                     'åŒ»ç”Ÿ',\n",
      "                     'åŒ»é™¢',\n",
      "                     'å¥',\n",
      "                     'åƒé¥­',\n",
      "                     'åŒå­¦',\n",
      "                     'åå­—',\n",
      "                     'åé¢',\n",
      "                     'å“ªå„¿',\n",
      "                     'å“ªé‡Œ',\n",
      "                     'å•†åº—',\n",
      "                     'å–œæ¬¢',\n",
      "                     'å¤©æ°”',\n",
      "                     'å¥³å„¿',\n",
      "                     'å¦ˆå¦ˆ',\n",
      "                     'å­¦ä¹ ',\n",
      "                     'å­¦æ ¡',\n",
      "                     'å­¦ç”Ÿ',\n",
      "                     'å®¢æ°”',\n",
      "                     'å¯¹ä¸èµ·',\n",
      "                     'å°å§',\n",
      "                     'å·¥ä½œ',\n",
      "                     'åº—é‡Œ',\n",
      "                     'æ€ä¹ˆ',\n",
      "                     'æ€ä¹ˆæ ·',\n",
      "                     'æˆ‘ä»¬',\n",
      "                     'æ‰“ç”µè¯',\n",
      "                     'æ—¶å€™',\n",
      "                     'æ˜å¤©',\n",
      "                     'æ˜ŸæœŸ',\n",
      "                     'æ˜¨å¤©',\n",
      "                     'æœ‰æ—¶å€™',\n",
      "                     'æœ‹å‹',\n",
      "                     'æ¯å­',\n",
      "                     'æ¡Œå­',\n",
      "                     'æ¤…å­',\n",
      "                     'æ°´æœ',\n",
      "                     'æ±‰è¯­',\n",
      "                     'æ²¡å…³ç³»',\n",
      "                     'æ²¡æœ‰',\n",
      "                     'æ¼‚äº®',\n",
      "                     'çˆ¸çˆ¸',\n",
      "                     'ç°åœ¨',\n",
      "                     'ç”µå½±',\n",
      "                     'ç”µè„‘',\n",
      "                     'ç”µè§†',\n",
      "                     'çœ‹è§',\n",
      "                     'ç¡è§‰',\n",
      "                     'ç±³é¥­',\n",
      "                     'è€å¸ˆ',\n",
      "                     'è‹¹æœ',\n",
      "                     'è¡£æœ',\n",
      "                     'è§',\n",
      "                     'è®¤ä¸º',\n",
      "                     'è®¤è¯†',\n",
      "                     'è¯è¯­',\n",
      "                     'è°¢è°¢',\n",
      "                     'è½¦è½¦',\n",
      "                     'é€ å¥',\n",
      "                     'é‡Œé¢',\n",
      "                     'é£æœº',\n",
      "                     'é¥­åº—',\n",
      "                     'é«˜å…´']},\n",
      " {'avg_unknown_tokens_per_sentence': 3.662251655629139,\n",
      "  'full_vocab_coverage': 0.0,\n",
      "  'model_path': '../models/grid_search/run_1/checkpoint-312',\n",
      "  'target_word_coverage': 0.9867549668874173,\n",
      "  'unknown_tokens': ['ä¸€ç‚¹å„¿',\n",
      "                     'ä¸€èµ·',\n",
      "                     'ä¸Šåˆ',\n",
      "                     'ä¸‹åˆ',\n",
      "                     'ä¸‹é›¨',\n",
      "                     'ä¸œè¥¿',\n",
      "                     'ä¸­åˆ',\n",
      "                     'ä¸­å›½',\n",
      "                     'äººé€ ',\n",
      "                     'ä»€ä¹ˆ',\n",
      "                     'ä»Šå¤©',\n",
      "                     'ä»–ä»¬',\n",
      "                     'ä½ ä»¬',\n",
      "                     'å„¿å­',\n",
      "                     'å…ˆç”Ÿ',\n",
      "                     'å†è§',\n",
      "                     'å‡ºç§Ÿè½¦',\n",
      "                     'åˆ†é’Ÿ',\n",
      "                     'å‰é¢',\n",
      "                     'åŒ—äº¬',\n",
      "                     'åŒ»ç”Ÿ',\n",
      "                     'åŒ»é™¢',\n",
      "                     'å¥',\n",
      "                     'åŒå­¦',\n",
      "                     'åå­—',\n",
      "                     'åé¢',\n",
      "                     'å“ªå„¿',\n",
      "                     'å“ªé‡Œ',\n",
      "                     'å•†åº—',\n",
      "                     'å–œæ¬¢',\n",
      "                     'å¤©æ°”',\n",
      "                     'å¥³å„¿',\n",
      "                     'å¦ˆå¦ˆ',\n",
      "                     'å­¦ä¹ ',\n",
      "                     'å­¦æ ¡',\n",
      "                     'å­¦ç”Ÿ',\n",
      "                     'å®¢æ°”',\n",
      "                     'å¯¹',\n",
      "                     'å¯¹ä¸èµ·',\n",
      "                     'å°å§',\n",
      "                     'å·¥ä½œ',\n",
      "                     'æ€ä¹ˆ',\n",
      "                     'æ€ä¹ˆæ ·',\n",
      "                     'æˆ‘ä»¬',\n",
      "                     'æ‰“ç”µè¯',\n",
      "                     'æ—¶å€™',\n",
      "                     'æ˜å¤©',\n",
      "                     'æ˜ŸæœŸ',\n",
      "                     'æ˜¨å¤©',\n",
      "                     'æœ‹å‹',\n",
      "                     'æ¯å­',\n",
      "                     'æ¡Œå­',\n",
      "                     'æ¤…å­',\n",
      "                     'æ°´æœ',\n",
      "                     'æ±‰è¯­',\n",
      "                     'æ²¡',\n",
      "                     'æ²¡å…³ç³»',\n",
      "                     'æ²¡æœ‰',\n",
      "                     'æ²¡ç‹—',\n",
      "                     'æ¼‚äº®',\n",
      "                     'çˆ¸çˆ¸',\n",
      "                     'ç°åœ¨',\n",
      "                     'ç”µå½±',\n",
      "                     'ç”µè„‘',\n",
      "                     'ç”µè§†',\n",
      "                     'çœ‹è§',\n",
      "                     'ç¡è§‰',\n",
      "                     'ç±³é¥­',\n",
      "                     'è€',\n",
      "                     'è€å¸ˆ',\n",
      "                     'è‹¹æœ',\n",
      "                     'è¡£æœ',\n",
      "                     'è®¤è¯†',\n",
      "                     'è¯è¯­',\n",
      "                     'è°¢è°¢',\n",
      "                     'è½¦',\n",
      "                     'é€ å¥',\n",
      "                     'é‡Œé¢',\n",
      "                     'é£æœº',\n",
      "                     'é¥­åº—',\n",
      "                     'é«˜å…´']},\n",
      " {'avg_unknown_tokens_per_sentence': 3.748344370860927,\n",
      "  'full_vocab_coverage': 0.0,\n",
      "  'model_path': '../models/grid_search/run_2/checkpoint-312',\n",
      "  'target_word_coverage': 0.9867549668874173,\n",
      "  'unknown_tokens': ['ä¸€ç‚¹å„¿',\n",
      "                     'ä¸‰æ¯æ°´',\n",
      "                     'ä¸Šåˆ',\n",
      "                     'ä¸‹åˆ',\n",
      "                     'ä¸‹é›¨',\n",
      "                     'ä¸œè¥¿',\n",
      "                     'ä¸­åˆ',\n",
      "                     'ä¸­å›½',\n",
      "                     'ä¸ºä»€ä¹ˆ',\n",
      "                     'äººé€ ',\n",
      "                     'ä»€ä¹ˆ',\n",
      "                     'ä»Šå¤©',\n",
      "                     'ä»–ä»¬',\n",
      "                     'ä½ ä»¬',\n",
      "                     'å„¿å­',\n",
      "                     'å…ˆç”Ÿ',\n",
      "                     'å†è§',\n",
      "                     'å‡ºå»',\n",
      "                     'å‡ºç§Ÿè½¦',\n",
      "                     'åˆ†é’Ÿ',\n",
      "                     'å‰é¢',\n",
      "                     'åŒ—äº¬',\n",
      "                     'åŒ»ç”Ÿ',\n",
      "                     'åŒ»é™¢',\n",
      "                     'å¥',\n",
      "                     'åƒé¥­',\n",
      "                     'åŒå­¦',\n",
      "                     'åå­—',\n",
      "                     'åé¢',\n",
      "                     'å“ªå„¿',\n",
      "                     'å“ªé‡Œ',\n",
      "                     'å•†åº—',\n",
      "                     'å–œæ¬¢',\n",
      "                     'å¤©æ°”',\n",
      "                     'å¥³å„¿',\n",
      "                     'å¦ˆå¦ˆ',\n",
      "                     'å­¦ä¹ ',\n",
      "                     'å­¦æ ¡',\n",
      "                     'å­¦ç”Ÿ',\n",
      "                     'å®¢æ°”',\n",
      "                     'å¯¹',\n",
      "                     'å¯¹ä¸èµ·',\n",
      "                     'å°å§',\n",
      "                     'å·¥ä½œ',\n",
      "                     'æ€ä¹ˆ',\n",
      "                     'æ€ä¹ˆæ ·',\n",
      "                     'æˆ‘ä»¬',\n",
      "                     'æ‰“ç”µè¯',\n",
      "                     'æ—¶å€™',\n",
      "                     'æ˜å¤©',\n",
      "                     'æ˜ŸæœŸ',\n",
      "                     'æ˜ŸæœŸä¸‰',\n",
      "                     'æ˜ŸæœŸå¤©',\n",
      "                     'æ˜¨å¤©',\n",
      "                     'æœ‹å‹',\n",
      "                     'æ¯å­',\n",
      "                     'æ ·å­',\n",
      "                     'æ¡Œå­',\n",
      "                     'æ¤…å­',\n",
      "                     'æ°´æœ',\n",
      "                     'æ±‰è¯­',\n",
      "                     'æ²¡',\n",
      "                     'æ²¡ä»€ä¹ˆ',\n",
      "                     'æ²¡å…³ç³»',\n",
      "                     'æ²¡æœ‰',\n",
      "                     'æ¼‚äº®',\n",
      "                     'çˆ¸çˆ¸',\n",
      "                     'ç°åœ¨',\n",
      "                     'ç”µå½±',\n",
      "                     'ç”µè„‘',\n",
      "                     'ç”µè§†',\n",
      "                     'çœ‹è§',\n",
      "                     'ç¡',\n",
      "                     'ç¡è§‰',\n",
      "                     'ç±³é¥­',\n",
      "                     'è€',\n",
      "                     'è€å¸ˆ',\n",
      "                     'è‹¹æœ',\n",
      "                     'è¡£æœ',\n",
      "                     'è§',\n",
      "                     'è®¤è¯†',\n",
      "                     'è¯è¯­',\n",
      "                     'è°¢è°¢',\n",
      "                     'è½¦å»',\n",
      "                     'è¿™é‡Œ',\n",
      "                     'é€ å¥',\n",
      "                     'é‚£å¤©',\n",
      "                     'é‡Œé¢',\n",
      "                     'é£æœº',\n",
      "                     'é¥­åº—',\n",
      "                     'é«˜',\n",
      "                     'é«˜å…´']}]\n"
     ]
    }
   ],
   "source": [
    "# Load HSK vocab\n",
    "from pprint import pprint\n",
    "\n",
    "hsk_vocab = load_hsk_vocab(\"../datasets/vocabulary/hsk1.txt\")\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    \"../models/grid_search/run_0/checkpoint-312\",\n",
    "    \"../models/grid_search/run_1/checkpoint-312\",\n",
    "    \"../models/grid_search/run_2/checkpoint-312\"\n",
    "]\n",
    "\n",
    "# cropped_set\n",
    "hsk_vocab = hsk_vocab\n",
    "\n",
    "# Evaluate\n",
    "reports = evaluate_models_on_hsk(models, hsk_vocab)\n",
    "\n",
    "pprint(reports)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
