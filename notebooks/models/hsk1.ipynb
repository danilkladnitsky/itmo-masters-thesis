{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (0.2.0)\n",
      "Requirement already satisfied: accelerate in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (1.6.0)\n",
      "Requirement already satisfied: jieba in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (0.42.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: filelock in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: pandas in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from datasets) (2024.12.0)\n",
      "Requirement already satisfied: psutil in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from accelerate) (2.6.0)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 KB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.11)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: networkx in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.18.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danilkladnitsky/.pyenv/versions/3.10.4/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.0 scikit-learn-1.6.1 threadpoolctl-3.6.0\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 25.1.1 is available.\n",
      "You should consider upgrading via the '/Users/danilkladnitsky/.pyenv/versions/3.10.4/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets sentencepiece accelerate jieba scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted 91774 labeled sentences to ../../train_datasets/txt/hsk3_labeled.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def extract_labeled_sentences(json_path, output_path):\n",
    "    \"\"\"\n",
    "    Extract labeled sentences from a JSON file and save them to a text file.\n",
    "    \n",
    "    Args:\n",
    "        json_path (str): Path to the input JSON file\n",
    "        output_path (str): Path to save the output text file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read JSON file\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Extract labeled sentences\n",
    "        labeled_sentences = [item['labeled_sentence'] for item in data if item['labeled_sentence']]\n",
    "        \n",
    "        # Write to output file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            for sentence in labeled_sentences:\n",
    "                f.write(sentence + '\\n')\n",
    "        \n",
    "        print(f\"Successfully extracted {len(labeled_sentences)} labeled sentences to {output_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file: {e}\")\n",
    "\n",
    "# Example usage\n",
    "TARGET_DATASET_PATH = \"../../train_datasets/json/hsk3.json\"\n",
    "OUTPUT_PATH = \"../../train_datasets/txt/hsk3_labeled.txt\"\n",
    "\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)\n",
    "\n",
    "# Extract labeled sentences\n",
    "extract_labeled_sentences(TARGET_DATASET_PATH, OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME=\"uer/gpt2-chinese-cluecorpussmall\"\n",
    "MODEL_PATH=\"../../models/hsk1-gpt2\"\n",
    "TARGET_DATASET_PATH = \"../../train_datasets/txt/hsk1_labeled.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'GPT2Tokenizer'.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers.tokenization_utils_base\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# === Load model and tokenizer ===\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muer/gpt2-chinese-cluecorpussmall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(BASE_MODEL_NAME)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mspecial_tokens_map)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2062\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2059\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2060\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2070\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2073\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2074\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2302\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2300\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2302\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[1;32m   2304\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2307\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/lib/python3.10/site-packages/transformers/models/gpt2/tokenization_gpt2.py:153\u001b[0m, in \u001b[0;36mGPT2Tokenizer.__init__\u001b[0;34m(self, vocab_file, merges_file, errors, unk_token, bos_token, eos_token, pad_token, add_prefix_space, add_bos_token, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m pad_token \u001b[38;5;241m=\u001b[39m AddedToken(pad_token, lstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, rstrip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pad_token, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m pad_token\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(vocab_handle)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, GPT2LMHeadModel,\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress tokenizer warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers.tokenization_utils_base\")\n",
    "\n",
    "# === Load model and tokenizer ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\", trust_remote_code=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# Make sure pad token is defined\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# === Load and prepare data ===\n",
    "with open(TARGET_DATASET_PATH, encoding=\"utf-8\") as f:\n",
    "    lines = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "train_lines, eval_lines = train_test_split(lines, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset = Dataset.from_list([{\"text\": l} for l in train_lines])\n",
    "eval_dataset = Dataset.from_list([{\"text\": l} for l in eval_lines])\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, max_length=64)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=False)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=False)\n",
    "\n",
    "# === Data collator ===\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# === Training arguments ===\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=MODEL_PATH,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,                      # âš ï¸ Reduced to avoid overfitting\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,                      # âš ï¸ Slightly lower for stability\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    eval_steps=100,                         # âœ… Evaluate every 100 steps\n",
    "    logging_steps=20,\n",
    "    logging_first_step=True,\n",
    "    prediction_loss_only=True,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_num_workers=2,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# === Trainer ===\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# === Train ===\n",
    "trainer.train()\n",
    "\n",
    "# === Save final model ===\n",
    "model.save_pretrained(MODEL_PATH)\n",
    "tokenizer.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]è¾“å…¥è¯è¯­ï¼šå•†åº—ã€‚ç”Ÿæˆå¥å­ï¼š[SEP]:æˆ‘åœ¨è¿™é‡Œä¹°ä¸œè¥¿ã€‚[SEP]æˆ‘åœ¨è¿™é‡Œä¹°ä¸œè¥¿ã€‚[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]ã€‚[SEP]ã€‚[SEP]ã€‚[SEP]ã€‚[SEP]ã€‚[SEP]ã€‚[SEP]ã€‚[SEP]ã€‚\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Ensure pad token is set\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "def generate_sentence(word, max_length=60):\n",
    "    prompt = f\"è¾“å…¥è¯è¯­ï¼š{word}ã€‚ç”Ÿæˆå¥å­ï¼š\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_length,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=0.5,\n",
    "            num_return_sequences=2,\n",
    "            pad_token_id=model.config.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(output_ids[0])\n",
    "    decoded = tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "    # Remove the prompt and unnecessary spaces\n",
    "    result = decoded.replace(prompt, \"\").replace(\" \", \"\").strip()\n",
    "\n",
    "    return result\n",
    "\n",
    "# ğŸ§ª Test the model with an HSK word\n",
    "print(generate_sentence(\"å•†åº—\"))\n",
    "\n",
    "# GENERATE SENTENCES WITH HSK WORDS\n",
    "\n",
    "# 1. CALCULATE COVERAGE\n",
    "# 2. CHECK GRAMMAR WITH SAPLING\n",
    "# 3. CHECK GRAMMAR WITH LLM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
